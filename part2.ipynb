{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489057c-0e87-47e8-97f7-0a630d6eadf9",
   "metadata": {
    "id": "1489057c-0e87-47e8-97f7-0a630d6eadf9"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d-7XBuJLEbb",
   "metadata": {
    "id": "2d-7XBuJLEbb"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"insert_your_token_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1778e89-05f3-4fbd-a4ff-5b5f19e36ea8",
   "metadata": {
    "id": "a1778e89-05f3-4fbd-a4ff-5b5f19e36ea8"
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185d897-cde9-4e05-99ba-5a19efd91597",
   "metadata": {
    "id": "b185d897-cde9-4e05-99ba-5a19efd91597"
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5f6a6-251f-486c-9991-12a04862a442",
   "metadata": {
    "id": "14e5f6a6-251f-486c-9991-12a04862a442"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943277e-f9a4-424c-aaa3-74332f61b725",
   "metadata": {
    "id": "b943277e-f9a4-424c-aaa3-74332f61b725"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_all_ranked.csv\")\n",
    "test_df = pd.read_csv(\"dev_all_ranked.csv\")\n",
    "#train_df = train_df.head(500)\n",
    "#test_df = test_df.head(500)\n",
    "hp_train_df = train_df.sample(frac = 0.1, random_state=42).reset_index()\n",
    "hp_test_df = test_df.sample(frac = 0.1, random_state=42).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b147cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)#['text']\n",
    "#test_df = test_df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1187d44-7724-4bbc-b33b-dcc002de74bd",
   "metadata": {
    "id": "d1187d44-7724-4bbc-b33b-dcc002de74bd"
   },
   "outputs": [],
   "source": [
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df[\"text\"] = self.df[\"ranked-sentences\"].progress_apply(lambda x:\" \".join(eval(x)[:10]))\n",
    "        #self.df[\"label\"] = self.df[\"decision\"].progress_apply(lambda x:1 if x==\"granted\" else 0)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        model_input = self.df['text'][idx]\n",
    "        encoded_sent = self.tokenizer.encode_plus(\n",
    "            text=model_input,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "            )\n",
    "\n",
    "        input_ids = encoded_sent.get('input_ids')\n",
    "        attention_mask = encoded_sent.get('attention_mask')\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        label = torch.tensor(self.df['label'][idx])\n",
    "\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57773094-f2dd-4aae-9eb9-ffb3837f37de",
   "metadata": {
    "id": "57773094-f2dd-4aae-9eb9-ffb3837f37de"
   },
   "outputs": [],
   "source": [
    "train_dataset = LegalDataset(train_df, tokenizer)\n",
    "test_dataset = LegalDataset(test_df, tokenizer)\n",
    "hp_train_dataset = LegalDataset(hp_train_df, tokenizer)\n",
    "hp_test_dataset = LegalDataset(hp_test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167dbcbc-389f-45d4-80e0-b765e1dfac19",
   "metadata": {
    "id": "167dbcbc-389f-45d4-80e0-b765e1dfac19"
   },
   "outputs": [],
   "source": [
    "metric1 = evaluate.load(\"accuracy\")\n",
    "metric2 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42716a-745e-4727-8e01-ea363abedea1",
   "metadata": {
    "id": "6c42716a-745e-4727-8e01-ea363abedea1"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric1.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric2.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {'accuracy': accuracy[\"accuracy\"], 'f1-score': f1[\"f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51569c0c-6377-403e-b2c9-ba33e8290bd9",
   "metadata": {
    "id": "51569c0c-6377-403e-b2c9-ba33e8290bd9"
   },
   "outputs": [],
   "source": [
    "def my_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\":trial.suggest_float(\"weight_decay\", 0.005, 0.05),\n",
    "        \"adam_beta1\":trial.suggest_float(\"adam_beta1\", 0.75, 0.95),\n",
    "        \"adam_beta2\":trial.suggest_float(\"adam_beta2\", 0.99, 0.9999),\n",
    "        \"adam_epsilon\":trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf81c2-18fe-4bde-b2c1-bb7b7a571c7a",
   "metadata": {
    "id": "46cf81c2-18fe-4bde-b2c1-bb7b7a571c7a"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='htf3_results',          # output directory\n",
    "    num_train_epochs=5,            # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,              # strength of weight decay\n",
    "    logging_dir='htf3_logs',           # directory for storing logs\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit = 1,\n",
    "    learning_rate = 0.00001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model =\"eval_f1-score\",\n",
    "    bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73102706-55c1-4c83-a6e1-34870dbfc1c9",
   "metadata": {
    "id": "73102706-55c1-4c83-a6e1-34870dbfc1c9"
   },
   "outputs": [],
   "source": [
    "# Create a list to store trial results\n",
    "trial_results = []\n",
    "\n",
    "class HPSearchCallback:\n",
    "    def __init__(self):\n",
    "        self.trial_number = 0\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        self.trial_number += 1\n",
    "        # Get the hyperparameters\n",
    "        params = trial.params\n",
    "        # Get the objective value (metric being optimized)\n",
    "        value = trial.value\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'trial': self.trial_number,\n",
    "            'hyperparameters': params,\n",
    "            'objective_value': value\n",
    "        }\n",
    "        trial_results.append(result)\n",
    "        \n",
    "        # Print results for this trial\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Trial {self.trial_number} Results:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Hyperparameters:\")\n",
    "        for key, val in params.items():\n",
    "            print(f\"  {key}: {val}\")\n",
    "        print(f\"\\nObjective Value (eval metric): {value:.4f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,                        # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=hp_train_dataset,         # training dataset\n",
    "    eval_dataset=hp_test_dataset,           # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861aa998-6334-498a-b2e8-f12a58039340",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "861aa998-6334-498a-b2e8-f12a58039340",
    "outputId": "74963e56-3cfc-4032-9791-701aef035d18"
   },
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    # Get hyperparameters\n",
    "    params = my_hp_space(trial)\n",
    "\n",
    "    # Store trial info\n",
    "    trial.set_user_attr('trial_number', trial.number + 1)\n",
    "\n",
    "    return params\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    n_trials=4,\n",
    "    direction=\"maximize\",\n",
    "    hp_space=optuna_hp_space,\n",
    ")\n",
    "\n",
    "# Extract and display trial results after search completes\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HYPERPARAMETER SEARCH COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Value: {best_run.objective:.4f}\")\n",
    "print(f\"  Hyperparameters:\")\n",
    "for key, val in best_run.hyperparameters.items():\n",
    "    print(f\"    {key}: {val}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be5278-a464-487d-b15d-04bbd6b0c927",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49be5278-a464-487d-b15d-04bbd6b0c927",
    "outputId": "b86375a4-1da9-41a6-8d44-edb5a146c083"
   },
   "outputs": [],
   "source": [
    "print(\"Best HyperParameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eaac8-878d-49d6-b75f-be7666736f67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "f33eaac8-878d-49d6-b75f-be7666736f67",
    "outputId": "3cfe037b-7e66-48c0-85af-9567da94720d"
   },
   "outputs": [],
   "source": [
    "# Fixed version - check if trial_results is empty first\n",
    "print(best_run)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF ALL TRIALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if trial_results:\n",
    "    for result in trial_results:\n",
    "        print(f\"\\nTrial {result['trial']}:\")\n",
    "        print(f\"  Hyperparameters: {result['hyperparameters']}\")\n",
    "        print(f\"  Objective Value: {result['objective_value']:.4f}\")\n",
    "\n",
    "    # Create a detailed dataframe\n",
    "    import pandas as pd\n",
    "    summary_data = []\n",
    "    for result in trial_results:\n",
    "        row = {'Trial': result['trial'], 'Objective_Value': result['objective_value']}\n",
    "        row.update(result['hyperparameters'])\n",
    "        summary_data.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Detailed Results Table:\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Show best trial\n",
    "    best_trial = max(trial_results, key=lambda x: x['objective_value'])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST TRIAL\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Trial Number: {best_trial['trial']}\")\n",
    "    print(f\"Objective Value: {best_trial['objective_value']:.4f}\")\n",
    "    print(f\"Hyperparameters: {best_trial['hyperparameters']}\")\n",
    "else:\n",
    "    print(\"No trial results were captured. Using best_run object instead.\")\n",
    "    print(f\"\\nBest Run ID: {best_run.run_id}\")\n",
    "    print(f\"Objective: {best_run.objective}\")\n",
    "    print(f\"Hyperparameters: {best_run.hyperparameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2bacb-b8b3-4ade-ad31-3df04cfff6b2",
   "metadata": {
    "id": "52d2bacb-b8b3-4ade-ad31-3df04cfff6b2"
   },
   "outputs": [],
   "source": [
    "del trainer\n",
    "del training_args\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d1868ea-2697-49e5-95ec-87038d6e4897",
   "metadata": {
    "id": "7d1868ea-2697-49e5-95ec-87038d6e4897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "016961c4-a64a-41a2-85b5-bc97d968ca2e",
   "metadata": {
    "id": "016961c4-a64a-41a2-85b5-bc97d968ca2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best hyperparameters from previous search:\n",
      "  learning_rate: 3.395690349549142e-05\n",
      "  weight_decay: 0.03261220631726104\n",
      "  adam_beta1: 0.8895833438683564\n",
      "  adam_beta2: 0.9930199195181635\n",
      "  adam_epsilon: 6.448086662178787e-09\n"
     ]
    }
   ],
   "source": [
    "# Define best hyperparameters from previous search\n",
    "best_hyperparameters = {\n",
    "    'learning_rate': 3.395690349549142e-05,\n",
    "    'weight_decay': 0.03261220631726104,\n",
    "    'adam_beta1': 0.8895833438683564,\n",
    "    'adam_beta2': 0.9930199195181635,\n",
    "    'adam_epsilon': 6.448086662178787e-09\n",
    "}\n",
    "\n",
    "print(\"Using best hyperparameters from previous search:\")\n",
    "for key, val in best_hyperparameters.items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "\n",
    "# Create training arguments with best hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='tf3_results',          # output directory\n",
    "    num_train_epochs=15,            # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=best_hyperparameters['weight_decay'],\n",
    "    logging_dir='tf3_logs',           # directory for storing logs\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=250,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    learning_rate=best_hyperparameters['learning_rate'],\n",
    "    adam_beta1=best_hyperparameters['adam_beta1'],\n",
    "    adam_beta2=best_hyperparameters['adam_beta2'],\n",
    "    adam_epsilon=best_hyperparameters['adam_epsilon'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1-score\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard if not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb13ccc5-14c5-4d51-ac66-1428260e6d02",
   "metadata": {
    "id": "bb13ccc5-14c5-4d51-ac66-1428260e6d02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpati\\AppData\\Local\\Temp\\ipykernel_21980\\2700080868.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create trainer without custom callback\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c54ab-3406-4d09-9742-ff1a95e45423",
   "metadata": {
    "id": "2d9c54ab-3406-4d09-9742-ff1a95e45423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arguments:\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.8895833438683564,\n",
      "adam_beta2=0.9930199195181635,\n",
      "adam_epsilon=6.448086662178787e-09,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3.395690349549142e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=tf3_logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=250,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_f1-score,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=15,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=tf3_results,\n",
      "overwrite_output_dir=False,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=500,\n",
      "weight_decay=0.03261220631726104,\n",
      ")\n",
      "\n",
      "Starting training with full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63014' max='232020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63014/232020 2:23:09 < 6:23:57, 7.34 it/s, Epoch 4.07/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.607988</td>\n",
       "      <td>0.704072</td>\n",
       "      <td>0.619837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611100</td>\n",
       "      <td>0.606761</td>\n",
       "      <td>0.704072</td>\n",
       "      <td>0.619837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598600</td>\n",
       "      <td>0.605344</td>\n",
       "      <td>0.704072</td>\n",
       "      <td>0.619837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.649404</td>\n",
       "      <td>0.461761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training arguments already include best hyperparameters, so just start training\n",
    "print(\"Training Arguments:\")\n",
    "print(trainer.args)\n",
    "print(\"\\nStarting training with full dataset...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b3b80-7013-4e1d-ae8f-54b10a68918a",
   "metadata": {
    "id": "c69b3b80-7013-4e1d-ae8f-54b10a68918a"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"/home2/username/legal-tech/tfidf_sum+indic-ad\")\n",
    "print(f\"Model saved to: /home2/username/legal-tech/tfidf_sum+indic-ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc1ada-9468-474c-b9d3-190ea806fabf",
   "metadata": {
    "id": "b5bc1ada-9468-474c-b9d3-190ea806fabf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
