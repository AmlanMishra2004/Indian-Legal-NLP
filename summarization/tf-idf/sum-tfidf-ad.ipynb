{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52f75c6-83b2-4b12-b9d1-6b7740e27420",
      "metadata": {
        "id": "b52f75c6-83b2-4b12-b9d1-6b7740e27420"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d8c714-fc23-489d-87d6-ff9d4c945312",
      "metadata": {
        "id": "70d8c714-fc23-489d-87d6-ff9d4c945312"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae4fdc6-1df1-40fc-b863-9ec9f4f0bc0f",
      "metadata": {
        "id": "5ae4fdc6-1df1-40fc-b863-9ec9f4f0bc0f"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "vectoriser = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hf auth login"
      ],
      "metadata": {
        "id": "o7Fhmc-3Rf82"
      },
      "id": "o7Fhmc-3Rf82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"Exploration-Lab/IL-TUR\", \"bail\")"
      ],
      "metadata": {
        "id": "f3u8Faf5Re-C"
      },
      "id": "f3u8Faf5Re-C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f03643f-9279-4c6f-a9a3-6ca9f47faeb9",
      "metadata": {
        "id": "5f03643f-9279-4c6f-a9a3-6ca9f47faeb9"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# import pandas as pd\n",
        "# def put_in_corpus(file):\n",
        "#   global corpus\n",
        "#   print(f\"Current file:{file}\")\n",
        "#   with open(f\"/scratch/username/train_test_data_for_modelling/{file}\") as f:\n",
        "#       data = json.load(f)\n",
        "#   df = pd.DataFrame(data)\n",
        "#   for i, row in tqdm(df.iterrows(),total=len(df)):\n",
        "#       src_sents=[]\n",
        "#       paras = df.loc[i][\"segments\"]['facts-and-arguments']\n",
        "#       for para in paras:\n",
        "#         sent = para.split('ред')\n",
        "#         sent = [i for i in sent if len(i)!=0 and i!=' ']\n",
        "#         src_sents.extend(sent)\n",
        "#       src_sents = list(filter(None, src_sents))\n",
        "#       src_sents1=[]\n",
        "#       for sent in src_sents:\n",
        "#         try:\n",
        "#           sent = ''.join([i for i in sent if not i.isdigit()])\n",
        "#         except:\n",
        "#           print(sent)\n",
        "#         src_sents1.append(sent)\n",
        "#       src_sents =src_sents1\n",
        "#       corpus.extend(src_sents)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def put_in_corpus_from_ds(split_name):\n",
        "    global corpus\n",
        "    dataset = ds[split_name]\n",
        "\n",
        "    print(f\"Processing split: {split_name}\")\n",
        "\n",
        "    for example in tqdm(dataset, total=len(dataset)):\n",
        "        src_sents = []\n",
        "\n",
        "        # text is a single long string\n",
        "        text = str(example[\"text\"]['facts-and-arguments'])\n",
        "\n",
        "        # split into sentences (Hindi danda)\n",
        "        sents = text.split('ред')\n",
        "        sents = [s for s in sents if s.strip()]\n",
        "\n",
        "        # remove digits\n",
        "        cleaned = []\n",
        "        for sent in sents:\n",
        "            sent = ''.join([c for c in sent if not c.isdigit()])\n",
        "            cleaned.append(sent)\n",
        "\n",
        "        corpus.extend(cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "W1TNSn0RSm03"
      },
      "id": "W1TNSn0RSm03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259711d9-6f1a-4463-8360-ec6585d0c474",
      "metadata": {
        "id": "259711d9-6f1a-4463-8360-ec6585d0c474"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/summaries_octtfidf2\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be428ff1-c318-4739-baef-7f48318039d7",
      "metadata": {
        "id": "be428ff1-c318-4739-baef-7f48318039d7"
      },
      "outputs": [],
      "source": [
        "put_in_corpus_from_ds('train_all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fca46dd-cf6a-4116-8e89-638794a711be",
      "metadata": {
        "id": "0fca46dd-cf6a-4116-8e89-638794a711be"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "def tok(s):\n",
        "  return s.split()\n",
        "cv = CountVectorizer(tokenizer=tok)\n",
        "data2 = cv.fit_transform(corpus)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_matrix = tfidf_transformer.fit_transform(data2)\n",
        "word2tfidf = dict(zip(cv.get_feature_names_out(), tfidf_transformer.idf_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c275f9-b34d-4791-8113-ea5051b1299e",
      "metadata": {
        "id": "a5c275f9-b34d-4791-8113-ea5051b1299e"
      },
      "outputs": [],
      "source": [
        "def get_score(sentence):\n",
        "  words = sentence.split()\n",
        "  score = 0\n",
        "  for word in words:\n",
        "    try:\n",
        "      score+= word2tfidf[word]\n",
        "    except:\n",
        "      pass\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f4fd61-fff0-4b1a-bc57-eaa41124bffd",
      "metadata": {
        "id": "f9f4fd61-fff0-4b1a-bc57-eaa41124bffd"
      },
      "outputs": [],
      "source": [
        "# def process(file):\n",
        "#     print(f\"Current file:{file}\")\n",
        "#     with open(f\"/scratch/username/train_test_data_for_modelling/{file}\") as f:\n",
        "#         data = json.load(f)\n",
        "#     df = pd.DataFrame(data)\n",
        "#     ranked_sentences = []\n",
        "#     for i, row in tqdm(df.iterrows(),total=len(df)):\n",
        "#         src_sents = df.loc[i][\"segments\"]['facts-and-arguments']\n",
        "#         src_sents = [i.split('ред') for i in src_sents]\n",
        "#         # split all paragraphs in individual sentences\n",
        "#         src_sents = [i for subl in src_sents for i in subl]\n",
        "#         src_sents = [i for i in src_sents if len(i)!=0 and i!=' ']\n",
        "#         src_sents = list(filter(None, src_sents))\n",
        "#         src_sents1=[]\n",
        "#         for sent in src_sents:\n",
        "#           try:\n",
        "#             sent = ''.join([i for i in sent if not i.isdigit()])\n",
        "#           except:\n",
        "#             print(sent)\n",
        "#           src_sents1.append(sent)\n",
        "#         src_sents =src_sents1\n",
        "#         sentences = src_sents\n",
        "#         scores=[]\n",
        "#         for sent in sentences:\n",
        "#           scores.append(get_score(sent))\n",
        "#         ranks = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "#         ranks = [i[1] for i in ranks]\n",
        "#         ranked_sentences.append(ranks)\n",
        "#     df = df.head(len(ranked_sentences))\n",
        "#     df['ranked-sentences'] = [i[:10] for i in ranked_sentences]\n",
        "#     df['ranked-sentences'].map(len)\n",
        "#     file = file.replace(\".json\",\".csv\")\n",
        "#     df.to_csv(f\"/scratch/username/summaries_octtfidf2/{file}\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def process_ds(split_name):\n",
        "    print(f\"Processing split: {split_name}\")\n",
        "\n",
        "    dataset = ds[split_name]\n",
        "    ranked_sentences = []\n",
        "\n",
        "    for example in tqdm(dataset, total=len(dataset)):\n",
        "        # 1. get full text\n",
        "        text = str(example[\"text\"]['facts-and-arguments'])\n",
        "\n",
        "        # 2. sentence split (Hindi danda)\n",
        "        src_sents = text.split('ред')\n",
        "\n",
        "        # 3. clean sentences\n",
        "        src_sents = [s.strip() for s in src_sents if s.strip()]\n",
        "\n",
        "        cleaned = []\n",
        "        for sent in src_sents:\n",
        "            sent = re.sub(r'\\d+', '', sent)   # remove digits\n",
        "            cleaned.append(sent)\n",
        "\n",
        "        sentences = cleaned\n",
        "\n",
        "        # 4. scoring\n",
        "        scores = []\n",
        "        for sent in sentences:\n",
        "            scores.append(get_score(sent))\n",
        "\n",
        "        # 5. ranking\n",
        "        ranks = sorted(\n",
        "            ((scores[i], s) for i, s in enumerate(sentences)),\n",
        "            reverse=True\n",
        "        )\n",
        "        ranks = [r[1] for r in ranks]\n",
        "\n",
        "        ranked_sentences.append(ranks)\n",
        "\n",
        "    # 6. convert to DataFrame for saving (to keep CSV output same)\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    df = df.head(len(ranked_sentences))\n",
        "    df[\"ranked-sentences\"] = [r[:10] for r in ranked_sentences]\n",
        "\n",
        "    out_file = f\"{split_name}_ranked.csv\"\n",
        "    df.to_csv(f\"{save_dir}/{out_file}\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ed6a49-5a7a-4abc-b305-7d1c7548d22b",
      "metadata": {
        "id": "54ed6a49-5a7a-4abc-b305-7d1c7548d22b"
      },
      "outputs": [],
      "source": [
        "splits = [\"train_all\",\"dev_all\",\"test_all\"]\n",
        "for split in splits:\n",
        "  process_ds(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c023ba4-c65d-414a-b8a9-dbf5e199b5e8",
      "metadata": {
        "id": "8c023ba4-c65d-414a-b8a9-dbf5e199b5e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}